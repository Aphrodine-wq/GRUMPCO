#!/bin/bash
# G-Rump GPU VM Quick Setup Script
# Run this on a fresh Ubuntu GPU VM to deploy G-Rump
#
# Usage:
#   curl -fsSL https://raw.githubusercontent.com/YOUR_ORG/g-rump/main/scripts/gpu-setup.sh | bash
#   # Or:
#   wget -qO- https://raw.githubusercontent.com/YOUR_ORG/g-rump/main/scripts/gpu-setup.sh | bash

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Banner
echo -e "${GREEN}"
cat << 'EOF'
   ____       ____                        
  / ___|     |  _ \ _   _ _ __ ___  _ __  
 | |  _ _____| |_) | | | | '_ ` _ \| '_ \ 
 | |_| |_____|  _ <| |_| | | | | | | |_) |
  \____|     |_| \_\\__,_|_| |_| |_| .__/ 
                                   |_|    
  GPU Deployment Setup Script
EOF
echo -e "${NC}"

# Check if running as root
if [[ $EUID -eq 0 ]]; then
   log_warn "Running as root. Will create non-root user for Docker."
fi

# Detect OS
if [[ -f /etc/os-release ]]; then
    . /etc/os-release
    OS=$ID
    VERSION=$VERSION_ID
else
    log_error "Cannot detect OS. Please use Ubuntu 20.04+ or Debian 11+"
    exit 1
fi

log_info "Detected OS: $OS $VERSION"

# ============================================================
# Step 1: System Updates
# ============================================================
log_info "Updating system packages..."
sudo apt update && sudo apt upgrade -y

# ============================================================
# Step 2: Install Docker
# ============================================================
if command -v docker &> /dev/null; then
    log_info "Docker already installed: $(docker --version)"
else
    log_info "Installing Docker..."
    curl -fsSL https://get.docker.com | sh
    sudo usermod -aG docker $USER
    log_success "Docker installed"
fi

# ============================================================
# Step 3: Install NVIDIA Container Toolkit
# ============================================================
if dpkg -l | grep -q nvidia-container-toolkit; then
    log_info "NVIDIA Container Toolkit already installed"
else
    log_info "Installing NVIDIA Container Toolkit..."
    
    # Add NVIDIA GPG key and repository
    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
        sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    
    sudo apt update
    sudo apt install -y nvidia-container-toolkit
    
    # Configure Docker to use NVIDIA runtime
    sudo nvidia-ctk runtime configure --runtime=docker
    sudo systemctl restart docker
    
    log_success "NVIDIA Container Toolkit installed"
fi

# ============================================================
# Step 4: Verify GPU Access
# ============================================================
log_info "Verifying GPU access in Docker..."
if docker run --rm --gpus all nvidia/cuda:12.2-base-ubuntu22.04 nvidia-smi > /dev/null 2>&1; then
    log_success "GPU accessible from Docker!"
    docker run --rm --gpus all nvidia/cuda:12.2-base-ubuntu22.04 nvidia-smi
else
    log_error "GPU not accessible. Check NVIDIA drivers."
    exit 1
fi

# ============================================================
# Step 5: Clone G-Rump Repository
# ============================================================
INSTALL_DIR="${GRUMP_INSTALL_DIR:-$HOME/g-rump}"

if [[ -d "$INSTALL_DIR" ]]; then
    log_info "G-Rump directory exists at $INSTALL_DIR"
    cd "$INSTALL_DIR"
    git pull origin main || true
else
    log_info "Cloning G-Rump to $INSTALL_DIR..."
    git clone https://github.com/YOUR_ORG/g-rump.git "$INSTALL_DIR"
    cd "$INSTALL_DIR"
fi

# ============================================================
# Step 6: Configure Environment
# ============================================================
log_info "Configuring environment..."

if [[ ! -f .env ]]; then
    # Prompt for API keys
    echo ""
    echo -e "${YELLOW}=== Configuration ===${NC}"
    echo ""
    
    read -p "Enter NVIDIA NIM API Key (from build.nvidia.com): " NVIDIA_API_KEY
    read -p "Enter NGC API Key (from ngc.nvidia.com, for local NIM): " NGC_API_KEY
    read -p "Run models locally on GPU? (y/n, default: y): " RUN_LOCAL
    RUN_LOCAL=${RUN_LOCAL:-y}
    
    cat > .env << EOF
# G-Rump GPU Configuration
# Generated by gpu-setup.sh on $(date)

NODE_ENV=production
PORT=3000
HOST=0.0.0.0

# NVIDIA API Keys
NVIDIA_NIM_API_KEY=${NVIDIA_API_KEY}
NGC_API_KEY=${NGC_API_KEY}

# Local NIM (GPU inference)
NVIDIA_NIM_URL=http://nim:8000/v1
GPU_ENABLED=true
NVIDIA_NIM_ENABLED=true

# Database
DB_TYPE=sqlite
DB_PATH=/app/data/grump.db

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# Performance tuning
WORKER_POOL_MIN=4
WORKER_POOL_MAX=16
NIM_EMBED_BATCH_SIZE=256
TIERED_CACHE_L1_MAX=2000
EOF
    
    log_success "Environment configured"
else
    log_info "Using existing .env file"
fi

# ============================================================
# Step 7: Determine GPU and Select Compose Profile
# ============================================================
log_info "Detecting GPU configuration..."

GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader | head -1)

log_info "GPU: $GPU_NAME"
log_info "VRAM: ${GPU_MEMORY}MB"
log_info "Count: $GPU_COUNT"

# Select appropriate compose file
if [[ $GPU_MEMORY -ge 40000 ]]; then
    COMPOSE_PROFILE="full"
    MODEL_SIZE="70B"
    log_info "Large GPU detected - using full 70B model"
elif [[ $GPU_MEMORY -ge 20000 ]]; then
    COMPOSE_PROFILE="standard"
    MODEL_SIZE="8B + embeddings"
    log_info "Medium GPU detected - using 8B model + embeddings"
else
    COMPOSE_PROFILE="minimal"
    MODEL_SIZE="8B quantized"
    log_warn "Small GPU detected - using quantized 8B model"
fi

# ============================================================
# Step 8: Login to NGC (for pulling NIM containers)
# ============================================================
if [[ -n "$NGC_API_KEY" ]]; then
    log_info "Logging into NVIDIA NGC..."
    echo "$NGC_API_KEY" | docker login nvcr.io -u '$oauthtoken' --password-stdin
    log_success "Logged into NGC"
fi

# ============================================================
# Step 9: Deploy
# ============================================================
log_info "Deploying G-Rump with GPU support..."
echo ""
echo -e "${YELLOW}Compose files:${NC}"
echo "  - deploy/docker-compose.yml (base)"
echo "  - deploy/docker-compose.gpu.yml (GPU overlay)"
echo ""

# Pull images first
docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml pull

# Start services
docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml up -d

# ============================================================
# Step 10: Wait for Services
# ============================================================
log_info "Waiting for services to start..."

# Wait for backend
echo -n "Waiting for backend..."
for i in {1..60}; do
    if curl -sf http://localhost:3000/health/live > /dev/null 2>&1; then
        echo -e " ${GREEN}Ready!${NC}"
        break
    fi
    echo -n "."
    sleep 2
done

# Wait for NIM (can take 2-5 minutes for model loading)
echo -n "Waiting for NIM (this may take a few minutes)..."
for i in {1..180}; do
    if curl -sf http://localhost:8000/v1/health/ready > /dev/null 2>&1; then
        echo -e " ${GREEN}Ready!${NC}"
        break
    fi
    echo -n "."
    sleep 2
done

# ============================================================
# Step 11: Verify Deployment
# ============================================================
echo ""
log_info "Verifying deployment..."

# Check containers
echo ""
echo -e "${YELLOW}Container Status:${NC}"
docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml ps

# Check GPU usage
echo ""
echo -e "${YELLOW}GPU Status:${NC}"
nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv

# Test NIM inference
echo ""
echo -e "${YELLOW}Testing NIM inference...${NC}"
RESPONSE=$(curl -s http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"model": "meta/llama-3.1-8b-instruct", "messages": [{"role": "user", "content": "Say hello in exactly 3 words"}], "max_tokens": 20}')

if echo "$RESPONSE" | grep -q "choices"; then
    log_success "NIM inference working!"
    echo "$RESPONSE" | jq -r '.choices[0].message.content' 2>/dev/null || echo "$RESPONSE"
else
    log_warn "NIM might still be loading. Check: docker logs grump-nim"
fi

# ============================================================
# Complete!
# ============================================================
echo ""
echo -e "${GREEN}============================================================${NC}"
echo -e "${GREEN}  G-Rump GPU Deployment Complete!${NC}"
echo -e "${GREEN}============================================================${NC}"
echo ""
echo -e "  ${BLUE}Frontend:${NC}    http://$(hostname -I | awk '{print $1}'):5173"
echo -e "  ${BLUE}Backend:${NC}     http://$(hostname -I | awk '{print $1}'):3000"
echo -e "  ${BLUE}NIM API:${NC}     http://$(hostname -I | awk '{print $1}'):8000"
echo ""
echo -e "  ${YELLOW}GPU:${NC}         $GPU_NAME (${GPU_MEMORY}MB)"
echo -e "  ${YELLOW}Model:${NC}       $MODEL_SIZE"
echo ""
echo -e "${YELLOW}Useful Commands:${NC}"
echo "  # View logs"
echo "  docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml logs -f"
echo ""
echo "  # Restart services"
echo "  docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml restart"
echo ""
echo "  # Stop all"
echo "  docker compose -f deploy/docker-compose.yml -f deploy/docker-compose.gpu.yml down"
echo ""
echo "  # Monitor GPU"
echo "  watch -n 1 nvidia-smi"
echo ""
echo -e "${GREEN}Happy coding! ðŸš€${NC}"
