# NeMo Framework fine-tuning config for G-Rump
# Run on NGC GPU VM (see README)

model:
  name: "meta-llama/Llama-3.2-1B"   # Small model for demo; use larger on multi-GPU
  peft_scheme: "lora"               # lora | none (full SFT)

training:
  output_dir: "./checkpoints/grump-sft"
  name: "grump_domain_sft"
  num_nodes: 1
  gpus_per_node: 1
  max_steps: 100

data:
  # Use synthetic Q&A from services/nemo-curator or SQuAD
  dataset: "rajpurkar/squad"
  split: "train"
