# ============================================================================
# QUICK START - Choose ONE option below
# ============================================================================
#
# OPTION 1: Mock Mode (zero-config, perfect for trying G-Rump immediately)
# No API key needed! Provides realistic AI placeholder responses.
# See: backend/.env.minimal for a quick-start template
#
MOCK_AI_MODE=false
#
# OPTION 2: Real AI - NVIDIA NIM (Powered by NVIDIA)
# Get your free API key at https://build.nvidia.com/
# ============================================================================

# ============================================================================
# AI Provider - NVIDIA NIM (Exclusive)
# ============================================================================
# Powered by NVIDIA NIM - https://build.nvidia.com/
# Get your free API key at https://build.nvidia.com (Nemotron, Llama, Mistral, and more)
#
NVIDIA_NIM_API_KEY=nvapi-your_key_here

# Local/self-hosted NIM base URL (e.g. http://nim:8000). When set, /v1 is appended if missing. Omit for cloud.
NVIDIA_NIM_URL=
# Embedding batch tuning (optional)
# NIM_EMBED_BATCH_SIZE: Number of embeddings per batch (default: 256, range: 1-512)
# NIM_EMBED_MAX_WAIT_MS: Max wait time to fill batch in milliseconds (default: 50, range: 10-500)
NIM_EMBED_BATCH_SIZE=
NIM_EMBED_MAX_WAIT_MS=

# NIM timeouts and retries (optional)
# Default timeout applies when model-specific values are not set.
# All timeout values are in MILLISECONDS. Recommended ranges:
#   - Default: 60000-180000 (1-3 minutes)
#   - 405B models: 120000-300000 (2-5 minutes, larger models need more time)
#   - Ultra/Super: 90000-180000 (1.5-3 minutes)
NIM_RETRY_ENABLED=true
NIM_TIMEOUT_DEFAULT_MS=120000
NIM_TIMEOUT_405B_MS=180000
NIM_TIMEOUT_ULTRA_MS=150000
NIM_TIMEOUT_SUPER_MS=120000

# Feature flags for risky routing (stage rollout: set to false to use previous models)
# USE_NEMOTRON_SWARM=false  → swarm uses Kimi instead of Nemotron Super
# USE_NEMOTRON_3_FOR_RAG=false  → long-context/RAG uses Llama instead of Nemotron 3 Nano 1M
USE_NEMOTRON_SWARM=
USE_NEMOTRON_3_FOR_RAG=

# Server
# Local development:
# NODE_ENV: "development" or "production"
# PORT: Server port (default: 3000, range: 1024-65535)
NODE_ENV=development
PORT=3000
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:5178,http://127.0.0.1:5178,tauri://localhost,http://tauri.localhost

# Figma Integration (Design-to-Code / Architecture mode – optional)
# Create an OAuth app at https://www.figma.com/developers/apps to get client id/secret.
# FIGMA_CLIENT_ID=
# FIGMA_CLIENT_SECRET=
# FIGMA_REDIRECT_URI=http://localhost:3000/api/figma/callback

#   NODE_ENV=production
#   CORS_ORIGINS=https://your-frontend-domain.com
#   EVENTS_MODE=poll

# Database
# - Local development: SQLite is fine and persists on disk.
DB_TYPE=sqlite
DB_PATH=./data/grump.db

# Redis (optional; rate limiting, cache, BullMQ)
REDIS_HOST=
# REDIS_PORT: Redis server port (default: 6379)
REDIS_PORT=6379
REDIS_PASSWORD=

# Abuse prevention – required in production when API is reachable by untrusted users (see docs/PRODUCTION.md)
# BLOCK_SUSPICIOUS_PROMPTS: Block requests containing prompt injection patterns (default: false = log only)
# REQUIRE_AUTH_FOR_API: Require authentication for /api/chat, /api/ship, /api/codegen endpoints
BLOCK_SUSPICIOUS_PROMPTS=false
REQUIRE_AUTH_FOR_API=false
SECURITY_STRICT_PROD=true

# Output filtering – redact sensitive content from API responses
# OUTPUT_FILTER_PII: Redact email addresses, SSN-like patterns (default: false)
# OUTPUT_FILTER_HARMFUL: Flag potentially harmful code patterns (default: false)
# STRICT_COMMAND_ALLOWLIST: Restrict bash_execute to allowed commands only (default: false)
OUTPUT_FILTER_PII=false
OUTPUT_FILTER_HARMFUL=false
STRICT_COMMAND_ALLOWLIST=false

# Host allowlist (comma-separated). Required in production when SECURITY_STRICT_PROD=true.
ALLOWED_HOSTS=localhost,127.0.0.1

# Webhooks – required in production if you use webhooks (inbound/outbound return 503 when unset)
GRUMP_WEBHOOK_SECRET=
GRUMP_WEBHOOK_URLS=

# Agent governance – block or strictly control Moltbot/OpenClaw and similar AI agents
# AGENT_ACCESS_POLICY: block (default) | allowlist | audit_only
# AGENT_ALLOWLIST: comma-separated agent IDs (only when allowlist policy)
# AGENT_RATE_LIMIT_PER_HOUR: max requests per allowlisted agent (default: 10, range: 1-1000)
# FREE_AGENT_ENABLED: when true, backend runs in full-agent mode (all tools, no approval gates for G-Rump app requests)
AGENT_ACCESS_POLICY=block
AGENT_ALLOWLIST=
AGENT_RATE_LIMIT_PER_HOUR=10
FREE_AGENT_ENABLED=false

# Intent compiler (optional; omit to use LLM-only fallback)
GRUMP_INTENT_PATH=
# Use WASM intent parser when available (true/1); fallback to CLI otherwise
GRUMP_USE_WASM_INTENT=
# Offload intent parsing to worker pool (true/1); worker runs CLI
GRUMP_USE_WORKER_POOL_INTENT=

# Auth – OAuth redirects (optional; defaults use PORT and localhost)
# PUBLIC_BASE_URL: Backend base URL for OAuth callback (e.g. http://localhost:3000)
# FRONTEND_URL: Frontend base URL after OAuth (e.g. http://localhost:5173)
# GITHUB_OAUTH_REDIRECT_URL / DISCORD_OAUTH_REDIRECT_URL: Override callback URLs if needed
PUBLIC_BASE_URL=http://localhost:3000
FRONTEND_URL=http://localhost:5173
# GITHUB_OAUTH_REDIRECT_URL=
# DISCORD_OAUTH_REDIRECT_URL=

# Supabase (leave as-is for mock mode, or replace with real credentials)
# SUPABASE_ANON_KEY used for Google/GitHub/Discord OAuth; enable providers in Supabase Dashboard
# SUPABASE_URL must be your project API URL (e.g. https://YOUR_PROJECT_REF.supabase.co), NOT the dashboard URL
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_KEY=your-service-key

# Serverless jobs (QStash)
QSTASH_TOKEN=
QSTASH_URL=
JOB_WORKER_SECRET=

# Stripe (for billing - optional; required in prod if billing used; webhook uses signature verification)
STRIPE_SECRET_KEY=sk_test_...
STRIPE_WEBHOOK_SECRET=whsec_...

# Twilio (optional; inbound SMS/voice/WhatsApp) – TWILIO_WEBHOOK_SECRET required in prod when MESSAGING_PROVIDER=twilio
MESSAGING_PROVIDER=
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_REPLY_TO_NUMBER=
TWILIO_WEBHOOK_SECRET=
# For WhatsApp: use same number with whatsapp: prefix, or set TWILIO_WHATSAPP_NUMBER=
TWILIO_WHATSAPP_NUMBER=

# Telegram (optional; native messaging – set webhook: https://api.telegram.org/bot{TOKEN}/setWebhook?url={PUBLIC_URL}/api/messaging/telegram)
TELEGRAM_BOT_TOKEN=
TELEGRAM_WEBHOOK_SECRET=

# Discord (optional; native messaging – requires non-serverless; enable Message Content intent in Discord Developer Portal)
# DISCORD_BOT_TOKEN already in env; bot starts when set

# Slack (optional; OAuth + Events API – connect workspace, chat from Slack)
SLACK_CLIENT_ID=
SLACK_CLIENT_SECRET=
SLACK_SIGNING_SECRET=
SLACK_BOT_TOKEN=

# GitHub (optional; webhook for PR/issue events – trigger SHIP from PR description)
GITHUB_WEBHOOK_SECRET=
# For GitHub App: GITHUB_APP_ID=, GITHUB_PRIVATE_KEY=

# Notion (optional; OAuth – sync PRD/specs to Notion, pull context)
NOTION_CLIENT_ID=
NOTION_CLIENT_SECRET=
NOTION_REDIRECT_URI=http://localhost:5173/notion/callback

# Docker Sandbox (for code execution - optional)
DOCKER_HOST=unix:///var/run/docker.sock
# SANDBOX_TIMEOUT_MS: Max execution time in milliseconds (default: 60000, range: 5000-300000)
SANDBOX_TIMEOUT_MS=60000
# SANDBOX_MEMORY_LIMIT: Container memory limit (e.g., 256m, 512m, 1g)
SANDBOX_MEMORY_LIMIT=512m

# Security scan root – restrict paths for /api/security/*; default is process.cwd()
SECURITY_SCAN_ROOT=

# Metrics (optional; Basic auth for /metrics – recommended in production)
METRICS_AUTH=

# Agent Lightning (optional; OpenTelemetry OTLP/HTTP endpoint for agent traces)
# Example: http://localhost:4747
OTLP_ENDPOINT=

# NVIDIA Observability - OpenTelemetry (NIM-compatible)
# Set OTEL_EXPORTER_OTLP_ENDPOINT to enable OTLP export for Datadog/Zipkin/Jaeger/Grafana
# OTEL_ENDPOINT may be used instead of OTLP_ENDPOINT (standard naming)
OTEL_EXPORTER_OTLP_ENDPOINT=
OTEL_METRICS_EXPORTER=otlp
OTEL_TRACES_EXPORTER=otlp
OTEL_SERVICE_NAME=grump-backend
# Agent Lightning task API (optional; for Free Agent task orchestration)
# AGENT_LIGHTNING_TASK_URL=
# AGENT_LIGHTNING_TASK_POLL_INTERVAL=60000

# Error Tracking (Sentry - optional; recommended for production)
# Get DSN from https://sentry.io/
SENTRY_DSN=
# Enable Sentry debug mode for development testing
SENTRY_DEBUG=false

# Alerting thresholds (optional; reduce noise on small instances / Supabase)
# ALERTING_HIGH_MEMORY_THRESHOLD: 0=disable, 85=default, 99=only at 99%
# ALERTING_HIGH_ERROR_RATE_THRESHOLD: 0=disable, 5=default %, 100=effectively disable
# ALERTING_DATABASE_FAILURE: false=skip DB connectivity check (use with Supabase)
# ALERTING_HIGH_MEMORY_THRESHOLD=99
# ALERTING_HIGH_ERROR_RATE_THRESHOLD=100
# ALERTING_DATABASE_FAILURE=false

# Rate Limiting Configuration (optional; defaults work well for most deployments)
# Override global rate limits:
# RATE_LIMIT_GLOBAL_WINDOW_MS=900000
# RATE_LIMIT_GLOBAL_MAX=100
# Override tier multipliers:
# RATE_LIMIT_TIER_MULTIPLIER_PRO=4
# RATE_LIMIT_TIER_MULTIPLIER_TEAM=8
# RATE_LIMIT_TIER_MULTIPLIER_ENTERPRISE=20
# Or provide full JSON config:
# RATE_LIMIT_CONFIG='{"global":{"windowMs":60000,"maxRequests":50},...}'

# API Versioning
# Set to 'true' to disable legacy unversioned routes (/api/...) and require /api/v1/...
# API_DISABLE_LEGACY_ROUTES=false

# RAG (Nemotron + file-based index; optional)
RAG_INDEX_PATH=./data/rag-index.json
RAG_LLM_MODEL=moonshotai/kimi-k2.5
REINDEX_SECRET=

# Voice (ASR + TTS via NVIDIA Build / NVCF; optional)
NVIDIA_BUILD_API_KEY=
NVIDIA_ASR_FUNCTION_ID=
NVIDIA_TTS_FUNCTION_ID=
# Or full URLs: NVIDIA_ASR_URL=, NVIDIA_TTS_URL=
# Base for function IDs: NVIDIA_BUILD_SPEECH_URL=https://api.nvcf.nvidia.com/v2/nvcf/exec/functions
