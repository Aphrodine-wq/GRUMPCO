import type { NvidiaModelConfig } from './types.js';
import { COST_TIER_LOW, COST_TIER_MEDIUM, COST_TIER_PREMIUM } from './types.js';

/**
 * Llama Family Models (Meta)
 */
export const LLAMA_MODELS: NvidiaModelConfig[] = [
  {
    id: 'meta/llama-4-maverick-17b-128e-instruct',
    name: 'Llama 4 Maverick 17B 128E',
    publisher: 'Meta',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'function-calling', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: COST_TIER_MEDIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_MEDIUM / 1_000_000,
    description:
      'General purpose multimodal, multilingual 128 MoE model with 17B active parameters',
    bestFor: ['chat', 'vision tasks', 'multilingual applications', 'agentic workflows'],
    parameters: '17B active (128 experts)',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['en', 'zh', 'es', 'fr', 'de', 'ja', 'ko', 'ar', 'hi'],
  },
  {
    id: 'meta/llama-4-scout-17b-16e-instruct',
    name: 'Llama 4 Scout 17B 16E',
    publisher: 'Meta',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: COST_TIER_LOW / 1_000_000,
    costPerTokenOutput: COST_TIER_LOW / 1_000_000,
    description: 'Multimodal, multilingual 16 MoE model with 17B parameters',
    bestFor: ['chat', 'vision tasks', 'lightweight deployment'],
    parameters: '17B (16 experts)',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-8b-instruct',
    name: 'Llama 3.1 8B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Lightweight, efficient model for edge and mobile deployment',
    bestFor: ['edge deployment', 'mobile apps', 'low-latency chat', 'basic coding'],
    parameters: '8B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-70b-instruct',
    name: 'Llama 3.1 70B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'High-performance model for complex tasks and reasoning',
    bestFor: ['complex reasoning', 'code generation', 'detailed analysis'],
    parameters: '70B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-405b-instruct',
    name: 'Llama 3.1 405B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: COST_TIER_PREMIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_PREMIUM / 1_000_000,
    description: 'Flagship Llama model with 405B parameters for maximum performance',
    bestFor: ['complex reasoning', 'advanced coding', 'research', 'enterprise tasks'],
    parameters: '405B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-guard-4-12b',
    name: 'Llama Guard 4 12B',
    publisher: 'Meta',
    capabilities: ['safety', 'multimodal'],
    contextWindow: 128_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Multi-modal safety model for content classification and moderation',
    bestFor: ['content moderation', 'safety filtering', 'input/output classification'],
    parameters: '12B',
    multimodal: true,
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'meta/llama-3.1-nemotron-nano-4b-v1.1',
    name: 'Llama 3.1 Nemotron Nano 4B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.01 / 1_000_000,
    costPerTokenOutput: 0.01 / 1_000_000,
    description:
      'State-of-the-art open model for edge agents - reasoning, code, math, tool calling',
    bestFor: ['edge deployment', 'mobile agents', 'embedded systems'],
    parameters: '4B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-nemotron-nano-8b-v1',
    name: 'Llama 3.1 Nemotron Nano 8B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.015 / 1_000_000,
    costPerTokenOutput: 0.015 / 1_000_000,
    description: 'Leading reasoning and agentic AI model for PC and edge deployment',
    bestFor: ['desktop agents', 'on-device AI', 'PC applications'],
    parameters: '8B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // CodeLlama Models
  {
    id: 'meta/codellama-7b-instruct',
    name: 'CodeLlama 7B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Lightweight code generation model for fast inference and edge deployment',
    bestFor: ['code completion', 'code explanation', 'lightweight coding', 'edge deployment'],
    parameters: '7B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-13b-instruct',
    name: 'CodeLlama 13B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.06 / 1_000_000,
    costPerTokenOutput: 0.06 / 1_000_000,
    description: 'Balanced code generation model with improved accuracy over 7B',
    bestFor: ['code generation', 'code review', 'bug fixing', 'refactoring'],
    parameters: '13B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-34b-instruct',
    name: 'CodeLlama 34B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'reasoning', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'High-performance code model with strong reasoning for complex programming tasks',
    bestFor: ['complex code generation', 'algorithm design', 'code architecture', 'debugging'],
    parameters: '34B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-70b-instruct',
    name: 'CodeLlama 70B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 100_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'Flagship CodeLlama model with 100K context for large codebase understanding',
    bestFor: [
      'enterprise code generation',
      'large codebase analysis',
      'system design',
      'advanced refactoring',
    ],
    parameters: '70B',
    supportsTools: true,
    supportsStreaming: true,
  },
];
