import type { NvidiaModelConfig } from './types.js';
import { COST_TIER_LOW as _COST_TIER_LOW, COST_TIER_MEDIUM } from './types.js';

/**
 * Mistral Family Models (Mistral AI)
 */
export const MISTRAL_MODELS: NvidiaModelConfig[] = [
  {
    id: 'mistralai/mistral-large-3-675b-instruct-2512',
    name: 'Mistral Large 3 675B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'function-calling', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: COST_TIER_MEDIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_MEDIUM / 1_000_000,
    description:
      'State-of-the-art general purpose MoE VLM ideal for chat, agentic and instruction-based use cases',
    bestFor: ['general chat', 'multimodal tasks', 'agentic workflows', 'vision+language'],
    parameters: '675B',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['en', 'fr', 'de', 'es', 'it', 'zh', 'ja'],
  },
  {
    id: 'mistralai/mistral-medium-3-instruct',
    name: 'Mistral Medium 3',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal', 'reasoning'],
    contextWindow: 128_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'Powerful, multimodal language model designed for enterprise applications',
    bestFor: ['enterprise apps', 'software development', 'data analysis'],
    parameters: '150B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mistral-small-24b-instruct',
    name: 'Mistral Small 24B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.1 / 1_000_000,
    costPerTokenOutput: 0.1 / 1_000_000,
    description: 'Latency-optimized language model excelling in code, math, general knowledge',
    bestFor: ['fast responses', 'coding tasks', 'general knowledge queries'],
    parameters: '24B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/ministral-14b-instruct-2512',
    name: 'Ministral 14B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal'],
    contextWindow: 128_000,
    costPerTokenInput: 0.08 / 1_000_000,
    costPerTokenOutput: 0.08 / 1_000_000,
    description: 'General purpose VLM ideal for chat and instruction-based use cases',
    bestFor: ['chatbots', 'visual QA', 'lightweight vision tasks'],
    parameters: '14B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/magistral-small-2506',
    name: 'Magistral Small',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'multilingual'],
    contextWindow: 32_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'High performance reasoning model optimized for efficiency and edge deployment',
    bestFor: ['edge reasoning', 'math problems', 'mobile deployment'],
    parameters: '7B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/devstral-2-123b-instruct-2512',
    name: 'Devstral 2 123B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: 0.8 / 1_000_000,
    costPerTokenOutput: 0.8 / 1_000_000,
    description: 'State-of-the-art open code model with deep reasoning and 256k context',
    bestFor: ['advanced coding', 'complex reasoning', 'large codebases'],
    parameters: '123B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mixtral-8x7b-instruct-v0.1',
    name: 'Mixtral 8x7B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning'],
    contextWindow: 32_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'Efficient MoE LLM that follows instructions and completes requests',
    bestFor: ['general chat', 'coding assistance', 'cost-effective reasoning'],
    parameters: '8x7B MoE',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mixtral-8x22b-instruct-v0.1',
    name: 'Mixtral 8x22B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'long-context'],
    contextWindow: 65_000,
    costPerTokenInput: 0.4 / 1_000_000,
    costPerTokenOutput: 0.4 / 1_000_000,
    description: 'Large MoE LLM with 8x22B experts for complex tasks',
    bestFor: ['complex reasoning', 'advanced coding', 'long documents'],
    parameters: '8x22B MoE',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
];
