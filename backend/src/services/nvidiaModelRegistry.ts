/**
 * NVIDIA Model Registry
 * 
 * Comprehensive registry of all available models from build.nvidia.com.
 * Includes all LLMs, VLMs, embeddings, and specialized models.
 * 
 * @see https://build.nvidia.com/models
 */

export type NvidiaModelCapability = 
  | 'chat'
  | 'code' 
  | 'vision'
  | 'multimodal'
  | 'reasoning'
  | 'embedding'
  | 'image-generation'
  | 'speech-to-text'
  | 'text-to-speech'
  | 'translation'
  | 'reranking'
  | 'safety'
  | 'agentic'
  | 'long-context'
  | 'function-calling'
  | 'tool-calling'
  | 'multilingual'
  | 'OCR'
  | 'text-generation'
  | 'math'
  | 'speech-recognition'
  | 'instruction-following'
  | 'summarization'
  | 'security'
  | '3d-generation'
  | 'image-to-3d'
  | 'text-to-3d'
  | 'video-generation'
  | 'physical-ai'
  | 'autonomous-vehicles'
  | 'end-to-end'
  | 'perception'
  | '3d-detection'
  | 'planning'
  | 'biology'
  | 'chemistry'
  | 'molecule-generation'
  | 'docking'
  | 'protein-folding'
  | 'drug-discovery'
  | 'protein-generation'
  | 'sequence-alignment'
  | 'speech-to-animation'
  | 'digital-humans'
  | 'speech-enhancement'
  | 'audio-processing'
  | 'OpenUSD';

export interface NvidiaModelConfig {
  id: string;
  name: string;
  publisher: string;
  capabilities: NvidiaModelCapability[];
  contextWindow: number;
  costPerTokenInput: number;
  costPerTokenOutput: number;
  description: string;
  bestFor: string[];
  parameters?: string;
  multimodal?: boolean;
  moE?: boolean;
  supportsTools: boolean;
  supportsStreaming: boolean;
  languages?: string[];
}

// Cost constants (per 1M tokens for consistency with industry standard)
const COST_TIER_LOW = 0.10;      // ~$0.10 per 1M tokens
const COST_TIER_MEDIUM = 0.60;   // ~$0.60 per 1M tokens  
const COST_TIER_HIGH = 1.00;     // ~$1.00 per 1M tokens
const COST_TIER_PREMIUM = 3.00;  // ~$3.00+ per 1M tokens

export const NVIDIA_MODEL_REGISTRY: NvidiaModelConfig[] = [
  // ==================== LLAMA MODELS ====================
  {
    id: 'meta/llama-4-maverick-17b-128e-instruct',
    name: 'Llama 4 Maverick 17B 128E',
    publisher: 'Meta',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'function-calling', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: COST_TIER_MEDIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_MEDIUM / 1_000_000,
    description: 'General purpose multimodal, multilingual 128 MoE model with 17B active parameters',
    bestFor: ['chat', 'vision tasks', 'multilingual applications', 'agentic workflows'],
    parameters: '17B active (128 experts)',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['en', 'zh', 'es', 'fr', 'de', 'ja', 'ko', 'ar', 'hi'],
  },
  {
    id: 'meta/llama-4-scout-17b-16e-instruct',
    name: 'Llama 4 Scout 17B 16E',
    publisher: 'Meta',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: COST_TIER_LOW / 1_000_000,
    costPerTokenOutput: COST_TIER_LOW / 1_000_000,
    description: 'Multimodal, multilingual 16 MoE model with 17B parameters',
    bestFor: ['chat', 'vision tasks', 'lightweight deployment'],
    parameters: '17B (16 experts)',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-8b-instruct',
    name: 'Llama 3.1 8B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Lightweight, efficient model for edge and mobile deployment',
    bestFor: ['edge deployment', 'mobile apps', 'low-latency chat', 'basic coding'],
    parameters: '8B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-70b-instruct',
    name: 'Llama 3.1 70B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'High-performance model for complex tasks and reasoning',
    bestFor: ['complex reasoning', 'code generation', 'detailed analysis'],
    parameters: '70B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-405b-instruct',
    name: 'Llama 3.1 405B',
    publisher: 'Meta',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 128_000,
    costPerTokenInput: COST_TIER_PREMIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_PREMIUM / 1_000_000,
    description: 'Flagship Llama model with 405B parameters for maximum performance',
    bestFor: ['complex reasoning', 'advanced coding', 'research', 'enterprise tasks'],
    parameters: '405B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-guard-4-12b',
    name: 'Llama Guard 4 12B',
    publisher: 'Meta',
    capabilities: ['safety', 'multimodal'],
    contextWindow: 128_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Multi-modal safety model for content classification and moderation',
    bestFor: ['content moderation', 'safety filtering', 'input/output classification'],
    parameters: '12B',
    multimodal: true,
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'meta/llama-3.1-nemotron-nano-4b-v1.1',
    name: 'Llama 3.1 Nemotron Nano 4B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.01 / 1_000_000,
    costPerTokenOutput: 0.01 / 1_000_000,
    description: 'State-of-the-art open model for edge agents - reasoning, code, math, tool calling',
    bestFor: ['edge deployment', 'mobile agents', 'embedded systems'],
    parameters: '4B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/llama-3.1-nemotron-nano-8b-v1',
    name: 'Llama 3.1 Nemotron Nano 8B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.015 / 1_000_000,
    costPerTokenOutput: 0.015 / 1_000_000,
    description: 'Leading reasoning and agentic AI model for PC and edge deployment',
    bestFor: ['desktop agents', 'on-device AI', 'PC applications'],
    parameters: '8B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== CODELLAMA MODELS ====================
  {
    id: 'meta/codellama-7b-instruct',
    name: 'CodeLlama 7B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Lightweight code generation model for fast inference and edge deployment',
    bestFor: ['code completion', 'code explanation', 'lightweight coding', 'edge deployment'],
    parameters: '7B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-13b-instruct',
    name: 'CodeLlama 13B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.06 / 1_000_000,
    costPerTokenOutput: 0.06 / 1_000_000,
    description: 'Balanced code generation model with improved accuracy over 7B',
    bestFor: ['code generation', 'code review', 'bug fixing', 'refactoring'],
    parameters: '13B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-34b-instruct',
    name: 'CodeLlama 34B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'reasoning', 'function-calling'],
    contextWindow: 16_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'High-performance code model with strong reasoning for complex programming tasks',
    bestFor: ['complex code generation', 'algorithm design', 'code architecture', 'debugging'],
    parameters: '34B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'meta/codellama-70b-instruct',
    name: 'CodeLlama 70B',
    publisher: 'Meta',
    capabilities: ['code', 'chat', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 100_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'Flagship CodeLlama model with 100K context for large codebase understanding',
    bestFor: ['enterprise code generation', 'large codebase analysis', 'system design', 'advanced refactoring'],
    parameters: '70B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== MISTRAL MODELS ====================
  {
    id: 'mistralai/mistral-large-3-675b-instruct-2512',
    name: 'Mistral Large 3 675B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal', 'agentic', 'function-calling', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: COST_TIER_MEDIUM / 1_000_000,
    costPerTokenOutput: COST_TIER_MEDIUM / 1_000_000,
    description: 'State-of-the-art general purpose MoE VLM ideal for chat, agentic and instruction-based use cases',
    bestFor: ['general chat', 'multimodal tasks', 'agentic workflows', 'vision+language'],
    parameters: '675B',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['en', 'fr', 'de', 'es', 'it', 'zh', 'ja'],
  },
  {
    id: 'mistralai/mistral-medium-3-instruct',
    name: 'Mistral Medium 3',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal', 'reasoning'],
    contextWindow: 128_000,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0.35 / 1_000_000,
    description: 'Powerful, multimodal language model designed for enterprise applications',
    bestFor: ['enterprise apps', 'software development', 'data analysis'],
    parameters: '150B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mistral-small-24b-instruct',
    name: 'Mistral Small 24B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Latency-optimized language model excelling in code, math, general knowledge',
    bestFor: ['fast responses', 'coding tasks', 'general knowledge queries'],
    parameters: '24B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/ministral-14b-instruct-2512',
    name: 'Ministral 14B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'vision', 'multimodal'],
    contextWindow: 128_000,
    costPerTokenInput: 0.08 / 1_000_000,
    costPerTokenOutput: 0.08 / 1_000_000,
    description: 'General purpose VLM ideal for chat and instruction-based use cases',
    bestFor: ['chatbots', 'visual QA', 'lightweight vision tasks'],
    parameters: '14B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/magistral-small-2506',
    name: 'Magistral Small',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'multilingual'],
    contextWindow: 32_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'High performance reasoning model optimized for efficiency and edge deployment',
    bestFor: ['edge reasoning', 'math problems', 'mobile deployment'],
    parameters: '7B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/devstral-2-123b-instruct-2512',
    name: 'Devstral 2 123B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: 0.80 / 1_000_000,
    costPerTokenOutput: 0.80 / 1_000_000,
    description: 'State-of-the-art open code model with deep reasoning and 256k context',
    bestFor: ['advanced coding', 'complex reasoning', 'large codebases'],
    parameters: '123B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mixtral-8x7b-instruct-v0.1',
    name: 'Mixtral 8x7B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning'],
    contextWindow: 32_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'Efficient MoE LLM that follows instructions and completes requests',
    bestFor: ['general chat', 'coding assistance', 'cost-effective reasoning'],
    parameters: '8x7B MoE',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'mistralai/mixtral-8x22b-instruct-v0.1',
    name: 'Mixtral 8x22B',
    publisher: 'Mistral AI',
    capabilities: ['chat', 'code', 'reasoning', 'long-context'],
    contextWindow: 65_000,
    costPerTokenInput: 0.40 / 1_000_000,
    costPerTokenOutput: 0.40 / 1_000_000,
    description: 'Large MoE LLM with 8x22B experts for complex tasks',
    bestFor: ['complex reasoning', 'advanced coding', 'long documents'],
    parameters: '8x22B MoE',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== DEEPSEEK MODELS ====================
  {
    id: 'deepseek-ai/deepseek-v3.2',
    name: 'DeepSeek V3.2',
    publisher: 'DeepSeek AI',
    capabilities: ['chat', 'reasoning', 'long-context', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.45 / 1_000_000,
    costPerTokenOutput: 0.45 / 1_000_000,
    description: '685B reasoning LLM with sparse attention and integrated agentic tools',
    bestFor: ['advanced reasoning', 'agentic workflows', 'long-context tasks'],
    parameters: '685B',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'deepseek-ai/deepseek-v3.1',
    name: 'DeepSeek V3.1',
    publisher: 'DeepSeek AI',
    capabilities: ['chat', 'reasoning', 'function-calling', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.40 / 1_000_000,
    costPerTokenOutput: 0.40 / 1_000_000,
    description: 'Hybrid AI model with fast reasoning and strong tool use',
    bestFor: ['general chat', 'tool use', 'reasoning tasks'],
    parameters: '685B',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'deepseek-ai/deepseek-v3.1-terminus',
    name: 'DeepSeek V3.1 Terminus',
    publisher: 'DeepSeek AI',
    capabilities: ['chat', 'reasoning', 'tool-calling', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.45 / 1_000_000,
    costPerTokenOutput: 0.45 / 1_000_000,
    description: 'Hybrid inference LLM with Think/Non-Think modes and strict function calling',
    bestFor: ['complex agents', 'structured reasoning', 'tool-intensive workflows'],
    parameters: '685B',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'deepseek-ai/deepseek-r1-distill-llama-8b',
    name: 'DeepSeek R1 Distill Llama 8B',
    publisher: 'DeepSeek AI',
    capabilities: ['chat', 'reasoning', 'code'],
    contextWindow: 128_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Distilled reasoning model based on Llama 3.1 8B',
    bestFor: ['edge reasoning', 'lightweight reasoning tasks', 'mobile'],
    parameters: '8B',
    supportsTools: false,
    supportsStreaming: true,
  },

  // ==================== NEMOTRON MODELS ====================
  {
    id: 'nvidia/llama-3.1-nemotron-ultra-253b-v1',
    name: 'Nemotron Ultra 253B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.60 / 1_000_000,
    costPerTokenOutput: 0.60 / 1_000_000,
    description: 'Superior inference efficiency with highest accuracy for scientific and complex reasoning',
    bestFor: ['scientific computing', 'complex math', 'enterprise coding'],
    parameters: '253B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/llama-3.3-nemotron-super-49b-v1.5',
    name: 'Nemotron Super 49B v1.5',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.20 / 1_000_000,
    costPerTokenOutput: 0.20 / 1_000_000,
    description: 'High efficiency model with leading accuracy for reasoning and tool calling',
    bestFor: ['balanced performance', 'cost-effective reasoning', 'general tasks'],
    parameters: '49B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/llama-3.3-nemotron-49b-instruct',
    name: 'Nemotron 49B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.18 / 1_000_000,
    costPerTokenOutput: 0.18 / 1_000_000,
    description: 'Efficient model for reasoning and instruction following',
    bestFor: ['general tasks', 'coding', 'instruction following'],
    parameters: '49B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/nvidia-nemotron-nano-9b-v2',
    name: 'Nemotron Nano 9B v2',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'reasoning', 'agentic'],
    contextWindow: 128_000,
    costPerTokenInput: 0.015 / 1_000_000,
    costPerTokenOutput: 0.015 / 1_000_000,
    description: 'High-efficiency LLM with hybrid Transformer-Mamba design',
    bestFor: ['edge reasoning', 'mobile', 'embedded systems'],
    parameters: '9B',
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/nemotron-3-nano-30b-a3b',
    name: 'Nemotron 3 Nano 30B A3B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'reasoning', 'function-calling', 'long-context'],
    contextWindow: 1_000_000,
    costPerTokenInput: 0.25 / 1_000_000,
    costPerTokenOutput: 0.25 / 1_000_000,
    description: 'Open, efficient MoE model with 1M context - coding, reasoning, tool calling',
    bestFor: ['very long documents', 'RAG', 'coding with large context'],
    parameters: '30B (A3B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== NEMOTRON VISION MODELS ====================
  {
    id: 'nvidia/nemotron-nano-12b-v2-vl',
    name: 'Nemotron Nano 12B v2 VL',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'vision', 'multimodal', 'reasoning'],
    contextWindow: 128_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Multi-image and video understanding with visual Q&A and summarization',
    bestFor: ['video understanding', 'multi-image analysis', 'visual QA'],
    parameters: '12B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/llama-3.1-nemotron-nano-vl-8b-v1',
    name: 'Nemotron Nano VL 8B',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'vision', 'multimodal', 'reasoning', 'OCR'],
    contextWindow: 128_000,
    costPerTokenInput: 0.08 / 1_000_000,
    costPerTokenOutput: 0.08 / 1_000_000,
    description: 'Multi-modal vision-language model for text/img understanding and OCR',
    bestFor: ['document intelligence', 'OCR', 'visual understanding', 'edge vision'],
    parameters: '8B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== KIMI MODELS ====================
  {
    id: 'moonshotai/kimi-k2.5',
    name: 'Kimi K2.5',
    publisher: 'Moonshot AI',
    capabilities: ['chat', 'code', 'vision', 'multimodal', 'reasoning', 'agentic', 'long-context', 'function-calling', 'embedding'],
    contextWindow: 256_000,
    costPerTokenInput: 0.60 / 1_000_000,
    costPerTokenOutput: 0.60 / 1_000_000,
    description: '1T parameter multimodal MoE for high-capacity video and image understanding',
    bestFor: ['multimodal tasks', 'video understanding', 'complex reasoning', 'agentic workflows'],
    parameters: '1T (MoE)',
    multimodal: true,
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['en', 'zh', 'ja', 'ko', 'es', 'fr', 'de'],
  },
  {
    id: 'moonshotai/kimi-k2-thinking',
    name: 'Kimi K2 Thinking',
    publisher: 'Moonshot AI',
    capabilities: ['chat', 'reasoning', 'function-calling', 'long-context', 'agentic'],
    contextWindow: 256_000,
    costPerTokenInput: 0.60 / 1_000_000,
    costPerTokenOutput: 0.60 / 1_000_000,
    description: 'Open reasoning model with 256K context and enhanced tool use',
    bestFor: ['step-by-step reasoning', 'complex problem solving', 'agent tasks'],
    parameters: '1T (MoE)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'moonshotai/kimi-k2-instruct',
    name: 'Kimi K2 Instruct',
    publisher: 'Moonshot AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'function-calling'],
    contextWindow: 256_000,
    costPerTokenInput: 0.60 / 1_000_000,
    costPerTokenOutput: 0.60 / 1_000_000,
    description: 'State-of-the-art open MoE model with strong reasoning and coding',
    bestFor: ['coding', 'complex reasoning', 'agentic applications'],
    parameters: '1T (MoE)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'moonshotai/kimi-k2-instruct-0905',
    name: 'Kimi K2 Instruct (0905)',
    publisher: 'Moonshot AI',
    capabilities: ['chat', 'code', 'reasoning', 'agentic', 'long-context', 'function-calling'],
    contextWindow: 512_000,
    costPerTokenInput: 0.70 / 1_000_000,
    costPerTokenOutput: 0.70 / 1_000_000,
    description: 'Enhanced version with longer context and advanced reasoning',
    bestFor: ['very long documents', 'advanced reasoning', 'agentic coding'],
    parameters: '1T (MoE)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== QWEN MODELS ====================
  {
    id: 'qwen/qwen3-coder-480b-a35b-instruct',
    name: 'Qwen3 Coder 480B A35B',
    publisher: 'Qwen',
    capabilities: ['chat', 'code', 'agentic', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: 0.80 / 1_000_000,
    costPerTokenOutput: 0.80 / 1_000_000,
    description: 'Excels in agentic coding and browser use with 256K context',
    bestFor: ['agentic coding', 'browser automation', 'large codebase analysis'],
    parameters: '480B (35B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'qwen/qwen3-235b-a22b',
    name: 'Qwen3 235B A22B',
    publisher: 'Qwen',
    capabilities: ['chat', 'reasoning', 'multilingual', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.50 / 1_000_000,
    costPerTokenOutput: 0.50 / 1_000_000,
    description: 'Advanced reasoning MoE model for multilingual tasks',
    bestFor: ['multilingual chat', 'reasoning', 'instruction following'],
    parameters: '235B (22B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'qwen/qwen3-next-80b-a3b-instruct',
    name: 'Qwen3 Next 80B A3B',
    publisher: 'Qwen',
    capabilities: ['chat', 'agentic', 'text-generation', 'function-calling'],
    contextWindow: 128_000,
    costPerTokenInput: 0.25 / 1_000_000,
    costPerTokenOutput: 0.25 / 1_000_000,
    description: 'Hybrid attention with sparse MoE for ultra-long context AI',
    bestFor: ['general chat', 'agent workflows', 'long-context tasks'],
    parameters: '80B (3B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'qwen/qwen3-next-80b-a3b-thinking',
    name: 'Qwen3 Next 80B A3B Thinking',
    publisher: 'Qwen',
    capabilities: ['chat', 'reasoning', 'text-generation'],
    contextWindow: 128_000,
    costPerTokenInput: 0.30 / 1_000_000,
    costPerTokenOutput: 0.30 / 1_000_000,
    description: '80B parameter AI model with hybrid reasoning supporting 119 languages',
    bestFor: ['multilingual reasoning', 'complex tasks', 'global applications'],
    parameters: '80B (3B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
    languages: ['119 languages supported'],
  },
  {
    id: 'qwen/qwq-32b',
    name: 'QWQ 32B',
    publisher: 'Qwen',
    capabilities: ['chat', 'code', 'reasoning', 'math'],
    contextWindow: 32_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Powerful reasoning model capable of thinking and reasoning',
    bestFor: ['mathematical reasoning', 'complex problem solving', 'coding'],
    parameters: '32B',
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'qwen/qwen2.5-coder-32b-instruct',
    name: 'Qwen2.5 Coder 32B',
    publisher: 'Qwen',
    capabilities: ['chat', 'code', 'text-generation'],
    contextWindow: 128_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'Advanced LLM for code generation and fixing across popular languages',
    bestFor: ['code generation', 'code repair', 'software development'],
    parameters: '32B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== GOOGLE GEMMA MODELS ====================
  {
    id: 'google/gemma-3n-e2b-it',
    name: 'Gemma 3N E2B IT',
    publisher: 'Google',
    capabilities: ['chat', 'vision', 'multimodal', 'speech-recognition'],
    contextWindow: 128_000,
    costPerTokenInput: 0.08 / 1_000_000,
    costPerTokenOutput: 0.08 / 1_000_000,
    description: 'Edge computing AI model accepting text, audio and image input',
    bestFor: ['edge deployment', 'multimodal edge apps', 'voice+vision tasks'],
    parameters: '2B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },
  {
    id: 'google/gemma-3n-e4b-it',
    name: 'Gemma 3N E4B IT',
    publisher: 'Google',
    capabilities: ['chat', 'vision', 'multimodal', 'speech-recognition'],
    contextWindow: 128_000,
    costPerTokenInput: 0.12 / 1_000_000,
    costPerTokenOutput: 0.12 / 1_000_000,
    description: 'Edge computing AI model with enhanced capabilities',
    bestFor: ['edge deployment', 'rich multimodal apps', 'mobile AI'],
    parameters: '4B',
    multimodal: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== OPENAI MODELS ====================
  {
    id: 'openai/gpt-oss-20b',
    name: 'GPT-OSS 20B',
    publisher: 'OpenAI',
    capabilities: ['chat', 'reasoning', 'math'],
    contextWindow: 128_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Smaller MoE text-only LLM for efficient reasoning and math',
    bestFor: ['efficient reasoning', 'mathematical tasks', 'cost-effective inference'],
    parameters: '20B (MoE)',
    moE: true,
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'openai/gpt-oss-120b',
    name: 'GPT-OSS 120B',
    publisher: 'OpenAI',
    capabilities: ['chat', 'reasoning', 'math'],
    contextWindow: 128_000,
    costPerTokenInput: 0.50 / 1_000_000,
    costPerTokenOutput: 0.50 / 1_000_000,
    description: 'MoE reasoning LLM designed to fit within 80GB GPU',
    bestFor: ['complex reasoning', 'research', 'on-premise deployment'],
    parameters: '120B (MoE)',
    moE: true,
    supportsTools: false,
    supportsStreaming: true,
  },

  // ==================== BYTEDANCE MODELS ====================
  {
    id: 'bytedance/seed-oss-36b-instruct',
    name: 'SEED-OSS 36B',
    publisher: 'ByteDance',
    capabilities: ['chat', 'reasoning', 'text-generation'],
    contextWindow: 128_000,
    costPerTokenInput: 0.20 / 1_000_000,
    costPerTokenOutput: 0.20 / 1_000_000,
    description: 'Open-source LLM with long-context, reasoning, and agentic intelligence',
    bestFor: ['long-document processing', 'reasoning', 'agent applications'],
    parameters: '36B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== MINIMAX MODELS ====================
  {
    id: 'minimaxai/minimax-m2',
    name: 'MiniMax M2',
    publisher: 'MiniMax',
    capabilities: ['chat', 'reasoning', 'function-calling', 'agentic', 'long-context'],
    contextWindow: 256_000,
    costPerTokenInput: 0.40 / 1_000_000,
    costPerTokenOutput: 0.40 / 1_000_000,
    description: 'Open Mixture of Experts LLM (230B, 10B active) for reasoning and tool-use',
    bestFor: ['agentic workflows', 'long-context reasoning', 'tool-intensive tasks'],
    parameters: '230B (10B active)',
    moE: true,
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== IBM MODELS ====================
  {
    id: 'ibm/granite-3.3-8b-instruct',
    name: 'Granite 3.3 8B',
    publisher: 'IBM',
    capabilities: ['chat', 'code', 'reasoning', 'instruction-following'],
    contextWindow: 128_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Small language model fine-tuned for reasoning, coding, and instruction-following',
    bestFor: ['enterprise coding', 'reliable instruction following', 'IBM ecosystem'],
    parameters: '8B',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== GLM MODELS ====================
  {
    id: 'z-ai/glm4.7',
    name: 'GLM-4.7',
    publisher: 'Z-AI',
    capabilities: ['chat', 'code', 'reasoning', 'tool-calling', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.25 / 1_000_000,
    costPerTokenOutput: 0.25 / 1_000_000,
    description: 'Multilingual agentic coding partner with stronger reasoning and tool use',
    bestFor: ['multilingual coding', 'tool use', 'UI development'],
    parameters: 'Unknown',
    supportsTools: true,
    supportsStreaming: true,
  },

  // ==================== THUDM MODELS ====================
  {
    id: 'thudm/chatglm3-6b',
    name: 'ChatGLM3 6B',
    publisher: 'THUDM',
    capabilities: ['chat', 'code', 'translation', 'multilingual'],
    contextWindow: 32_000,
    costPerTokenInput: 0.03 / 1_000_000,
    costPerTokenOutput: 0.03 / 1_000_000,
    description: 'Chinese and English model for chatbots, content generation, coding',
    bestFor: ['Chinese language tasks', 'translation', 'lightweight chat'],
    parameters: '6B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['zh', 'en'],
  },

  // ==================== SARVAM MODELS ====================
  {
    id: 'sarvamai/sarvam-m',
    name: 'Sarvam M',
    publisher: 'Sarvam AI',
    capabilities: ['chat', 'code', 'reasoning', 'math', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'Multilingual hybrid-reasoning model optimized for Indian languages',
    bestFor: ['Indian language tasks', 'multilingual apps', 'Indic programming'],
    parameters: 'Unknown',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['hi', 'ta', 'te', 'kn', 'ml', 'mr', 'bn', 'gu', 'pa', 'en'],
  },

  // ==================== REGIONAL/SOVEREIGN AI MODELS ====================
  {
    id: 'stockmark/stockmark-2-100b-instruct',
    name: 'Stockmark 2 100B',
    publisher: 'Stockmark',
    capabilities: ['chat', 'text-generation', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.45 / 1_000_000,
    costPerTokenOutput: 0.45 / 1_000_000,
    description: 'Japanese-specialized LLM for enterprise business documents',
    bestFor: ['Japanese business documents', 'enterprise Japan', 'sovereign AI'],
    parameters: '100B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['ja', 'en'],
  },
  {
    id: 'speakleash/bielik-11b-v2.6-instruct',
    name: 'Bielik 11B v2.6',
    publisher: 'SpeakLeash',
    capabilities: ['chat', 'text-generation', 'summarization'],
    contextWindow: 32_000,
    costPerTokenInput: 0.06 / 1_000_000,
    costPerTokenOutput: 0.06 / 1_000_000,
    description: 'State-of-the-art model for Polish language processing',
    bestFor: ['Polish language', 'Polish chatbots', 'sovereign AI (Poland)'],
    parameters: '11B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['pl', 'en'],
  },
  {
    id: 'opengpt-x/teuken-7b-instruct-commercial-v0.4',
    name: 'Teuken 7B Commercial',
    publisher: 'OpenGPT-X',
    capabilities: ['chat', 'text-generation', 'multilingual'],
    contextWindow: 32_000,
    costPerTokenInput: 0.04 / 1_000_000,
    costPerTokenOutput: 0.04 / 1_000_000,
    description: 'Multilingual 7B LLM instruction-tuned on all 24 EU languages',
    bestFor: ['EU languages', 'European applications', 'multilingual EU content'],
    parameters: '7B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['all 24 EU languages'],
  },
  {
    id: 'utter-project/eurollm-9b-instruct',
    name: 'EuroLLM 9B',
    publisher: 'Utter Project',
    capabilities: ['chat', 'text-generation', 'multilingual'],
    contextWindow: 32_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'State-of-the-art multilingual model for all 24 EU languages',
    bestFor: ['EU sovereign AI', 'multilingual Europe', 'regional content'],
    parameters: '9B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['all 24 EU languages'],
  },
  {
    id: 'gotocompany/gemma-2-9b-cpt-sahabatai-instruct',
    name: 'SahabatAI Gemma 2 9B',
    publisher: 'Goto Company',
    capabilities: ['chat', 'text-generation', 'instruction-following'],
    contextWindow: 32_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Indonesian-specialized LLM with proficiency in local dialects',
    bestFor: ['Indonesian language', 'local dialects', 'Southeast Asia'],
    parameters: '9B',
    supportsTools: true,
    supportsStreaming: true,
    languages: ['id', 'en', 'local dialects'],
  },

  // ==================== MICROSOFT MODELS ====================
  {
    id: 'microsoft/phi-4-mini-flash-reasoning',
    name: 'Phi-4 Mini Flash Reasoning',
    publisher: 'Microsoft',
    capabilities: ['chat', 'reasoning', 'text-generation', 'math'],
    contextWindow: 16_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Lightweight reasoning model for latency-bound, memory-constrained environments',
    bestFor: ['edge reasoning', 'mobile math', 'fast reasoning'],
    parameters: 'Unknown',
    supportsTools: false,
    supportsStreaming: true,
  },

  // ==================== EMBEDDING MODELS ====================
  {
    id: 'nvidia/nv-embed-v1',
    name: 'NV-Embed v1',
    publisher: 'NVIDIA',
    capabilities: ['embedding'],
    contextWindow: 512,
    costPerTokenInput: 0.01 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Generates high-quality numerical embeddings from text inputs',
    bestFor: ['text embeddings', 'semantic search', 'RAG'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nv-embedqa-e5-v5',
    name: 'NV-EmbedQA E5 v5',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'reranking'],
    contextWindow: 512,
    costPerTokenInput: 0.01 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'English text embedding model for question-answering retrieval',
    bestFor: ['QA retrieval', 'English embeddings', 'RAG'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/llama-3_2-nemoretriever-300m-embed-v1',
    name: 'NeMo Retriever 300M Embed v1',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'multilingual'],
    contextWindow: 512,
    costPerTokenInput: 0.005 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Multilingual, cross-lingual embedding model supporting 26 languages',
    bestFor: ['multilingual RAG', 'cross-lingual search', 'long-document QA'],
    supportsTools: false,
    supportsStreaming: false,
    languages: ['26 languages'],
  },
  {
    id: 'nvidia/llama-3_2-nemoretriever-300m-embed-v2',
    name: 'NeMo Retriever 300M Embed v2',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'multilingual'],
    contextWindow: 512,
    costPerTokenInput: 0.005 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Enhanced multilingual embedding model with improved accuracy',
    bestFor: ['multilingual RAG', 'cross-lingual search', 'improved embeddings'],
    supportsTools: false,
    supportsStreaming: false,
    languages: ['26 languages'],
  },
  {
    id: 'nvidia/llama-3_2-nemoretriever-1b-vlm-embed-v1',
    name: 'NeMo Retriever 1B VLM Embed v1',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'multimodal'],
    contextWindow: 512,
    costPerTokenInput: 0.015 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Multimodal question-answer retrieval representing queries as text and documents as images',
    bestFor: ['visual document retrieval', 'multimodal RAG', 'image+text search'],
    multimodal: true,
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/llama-3.2-nv-embedqa-1b-v2',
    name: 'NV-EmbedQA 1B v2',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'multilingual'],
    contextWindow: 512,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Multilingual and cross-lingual text QA retrieval with long context support',
    bestFor: ['QA retrieval', 'long context', 'multilingual search'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/llama-3.2-nv-rerankqa-1b-v2',
    name: 'NV-RerankQA 1B v2',
    publisher: 'NVIDIA',
    capabilities: ['reranking', 'multilingual'],
    contextWindow: 512,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Fine-tuned reranking model for multilingual text question-answering',
    bestFor: ['result reranking', 'QA optimization', 'search improvement'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/llama-3.2-nemoretriever-500m-rerank-v2',
    name: 'NeMo Retriever 500M Rerank v2',
    publisher: 'NVIDIA',
    capabilities: ['reranking'],
    contextWindow: 512,
    costPerTokenInput: 0.01 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'GPU-accelerated model for passage relevance scoring',
    bestFor: ['passage ranking', 'retrieval optimization', 'RAG improvement'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== SAFETY/SECURITY MODELS ====================
  {
    id: 'nvidia/llama-3.1-nemotron-safety-guard-8b-v3',
    name: 'Nemotron Safety Guard 8B',
    publisher: 'NVIDIA',
    capabilities: ['safety', 'multilingual'],
    contextWindow: 128_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Leading multilingual content safety model for LLM safety and moderation',
    bestFor: ['content moderation', 'safety guardrails', 'multilingual safety'],
    parameters: '8B',
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nemotron-content-safety-reasoning-4b',
    name: 'Nemotron Content Safety Reasoning 4B',
    publisher: 'NVIDIA',
    capabilities: ['safety', 'reasoning'],
    contextWindow: 32_000,
    costPerTokenInput: 0.03 / 1_000_000,
    costPerTokenOutput: 0.03 / 1_000_000,
    description: 'Context-aware safety model that applies reasoning to enforce domain-specific policies',
    bestFor: ['policy enforcement', 'context-aware moderation', 'reasoning safety'],
    parameters: '4B',
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nemoguard-jailbreak-detect',
    name: 'NeMo Guard Jailbreak Detect',
    publisher: 'NVIDIA',
    capabilities: ['safety', 'security'],
    contextWindow: 8_000,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Industry-leading jailbreak classification model for adversarial protection',
    bestFor: ['jailbreak detection', 'prompt injection defense', 'security'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== DOCUMENT INTELLIGENCE MODELS ====================
  {
    id: 'nvidia/nemoretriever-ocr-v1',
    name: 'NeMo Retriever OCR',
    publisher: 'NVIDIA',
    capabilities: ['OCR', 'vision'],
    contextWindow: 4_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Powerful OCR model for fast, accurate real-world image text extraction',
    bestFor: ['document OCR', 'text extraction', 'layout analysis'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nemoretriever-page-elements-v3',
    name: 'NeMo Retriever Page Elements v3',
    publisher: 'NVIDIA',
    capabilities: ['vision', 'OCR'],
    contextWindow: 4_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Object detection model fine-tuned for charts, tables, and titles in documents',
    bestFor: ['document layout', 'chart detection', 'table extraction'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nemotron-parse',
    name: 'Nemotron Parse',
    publisher: 'NVIDIA',
    capabilities: ['vision', 'OCR', 'multimodal'],
    contextWindow: 8_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Vision-language model excelling in retrieving text and metadata from images',
    bestFor: ['document parsing', 'metadata extraction', 'structured documents'],
    multimodal: true,
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/nv-yolox-page-elements-v1',
    name: 'NV-YOLOX Page Elements',
    publisher: 'NVIDIA',
    capabilities: ['vision', 'OCR'],
    contextWindow: 4_000,
    costPerTokenInput: 0.04 / 1_000_000,
    costPerTokenOutput: 0.04 / 1_000_000,
    description: 'Object detection for charts, tables, and titles in documents',
    bestFor: ['document analysis', 'visual extraction', 'layout parsing'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'baidu/paddleocr',
    name: 'PaddleOCR',
    publisher: 'Baidu',
    capabilities: ['OCR', 'vision'],
    contextWindow: 4_000,
    costPerTokenInput: 0.03 / 1_000_000,
    costPerTokenOutput: 0.03 / 1_000_000,
    description: 'Table extraction with OCR, returning text and bounding boxes',
    bestFor: ['table extraction', 'Chinese OCR', 'document processing'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== SPEECH MODELS ====================
  {
    id: 'nvidia/parakeet-tdt-0.6b-v2',
    name: 'Parakeet TDT 0.6B',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Accurate and optimized English transcriptions with punctuation and timestamps',
    bestFor: ['English ASR', 'word timestamps', 'punctuation'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['en'],
  },
  {
    id: 'nvidia/parakeet-ctc-0.6b-asr',
    name: 'Parakeet CTC 0.6B ASR',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Accurate English transcription with streaming support',
    bestFor: ['streaming ASR', 'English speech', 'real-time transcription'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['en'],
  },
  {
    id: 'nvidia/parakeet-ctc-1.1b-asr',
    name: 'Parakeet CTC 1.1B ASR',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.003 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Record-setting accuracy and performance for English transcription',
    bestFor: ['high-accuracy ASR', 'batch transcription', 'English audio'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['en'],
  },
  {
    id: 'nvidia/parakeet-ctc-0.6b-zh-cn',
    name: 'Parakeet CTC 0.6B Mandarin',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Accurate and optimized Mandarin-English transcriptions',
    bestFor: ['Mandarin ASR', 'Chinese speech', 'bilingual transcription'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['zh-cn', 'en'],
  },
  {
    id: 'nvidia/parakeet-ctc-0.6b-zh-tw',
    name: 'Parakeet CTC 0.6B Taiwanese',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Record-setting accuracy for Mandarin-Taiwanese-English transcriptions',
    bestFor: ['Taiwanese ASR', 'Mandarin+Taiwanese', 'regional Chinese'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['zh-tw', 'zh-cn', 'en'],
  },
  {
    id: 'nvidia/parakeet-ctc-0.6b-es',
    name: 'Parakeet CTC 0.6B Spanish',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Accurate Spanish-English transcriptions with punctuation',
    bestFor: ['Spanish ASR', 'bilingual transcription', 'Hispanic markets'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['es', 'en'],
  },
  {
    id: 'nvidia/parakeet-ctc-0.6b-vi',
    name: 'Parakeet CTC 0.6B Vietnamese',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-text'],
    contextWindow: 0,
    costPerTokenInput: 0.002 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Accurate Vietnamese-English transcriptions with punctuation',
    bestFor: ['Vietnamese ASR', 'Southeast Asia', 'bilingual transcription'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['vi', 'en'],
  },
  {
    id: 'nvidia/magpie-tts-flow',
    name: 'Magpie TTS Flow',
    publisher: 'NVIDIA',
    capabilities: ['text-to-speech'],
    contextWindow: 4_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Expressive and engaging text-to-speech from short audio samples',
    bestFor: ['voice cloning', 'expressive TTS', 'brand voices'],
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/magpie-tts-multilingual',
    name: 'Magpie TTS Multilingual',
    publisher: 'NVIDIA',
    capabilities: ['text-to-speech', 'multilingual'],
    contextWindow: 4_000,
    costPerTokenInput: 0.06 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Natural and expressive voices in multiple languages',
    bestFor: ['multilingual TTS', 'voice agents', 'global applications'],
    supportsTools: false,
    supportsStreaming: true,
    languages: ['en', 'es', 'fr', 'de', 'it', 'ja', 'zh', 'ko'],
  },
  {
    id: 'nvidia/riva-translate-1.6b',
    name: 'Riva Translate 1.6B',
    publisher: 'NVIDIA',
    capabilities: ['translation'],
    contextWindow: 4_000,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.05 / 1_000_000,
    description: 'Smooth global interactions in 36 languages',
    bestFor: ['multilingual translation', 'global communication', 'NMT'],
    supportsTools: false,
    supportsStreaming: false,
    languages: ['36 languages'],
  },
  {
    id: 'nvidia/riva-translate-4b-instruct-v1_1',
    name: 'Riva Translate 4B Instruct v1.1',
    publisher: 'NVIDIA',
    capabilities: ['translation'],
    contextWindow: 8_000,
    costPerTokenInput: 0.08 / 1_000_000,
    costPerTokenOutput: 0.08 / 1_000_000,
    description: 'Translation in 12 languages with few-shot example capability',
    bestFor: ['instruction-based translation', 'customizable NMT', '12 languages'],
    supportsTools: false,
    supportsStreaming: false,
    languages: ['12 languages'],
  },

  // ==================== VISION/PHYSICAL AI MODELS ====================
  {
    id: 'nvidia/cosmos-reason1-7b',
    name: 'Cosmos Reason1 7B',
    publisher: 'NVIDIA',
    capabilities: ['vision', 'reasoning', 'multimodal'],
    contextWindow: 8_000,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0.10 / 1_000_000,
    description: 'Reasoning vision language model for physical AI and robotics',
    bestFor: ['robotics', 'physical AI', 'video understanding', 'industrial'],
    parameters: '7B',
    multimodal: true,
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/cosmos-reason2-8b',
    name: 'Cosmos Reason2 8B',
    publisher: 'NVIDIA',
    capabilities: ['vision', 'reasoning', 'multimodal'],
    contextWindow: 8_000,
    costPerTokenInput: 0.12 / 1_000_000,
    costPerTokenOutput: 0.12 / 1_000_000,
    description: 'Vision language model for structured reasoning on videos or images',
    bestFor: ['video reasoning', 'synthetic data generation', 'autonomous vehicles'],
    parameters: '8B',
    multimodal: true,
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/nvclip',
    name: 'NV-CLIP',
    publisher: 'NVIDIA',
    capabilities: ['embedding', 'vision', 'multimodal'],
    contextWindow: 77,
    costPerTokenInput: 0.02 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Multimodal embeddings model for image and text',
    bestFor: ['image-text similarity', 'multimodal search', 'visual embeddings'],
    multimodal: true,
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== IMAGE GENERATION MODELS ====================
  {
    id: 'stabilityai/stable-diffusion-3.5-large',
    name: 'Stable Diffusion 3.5 Large',
    publisher: 'Stability AI',
    capabilities: ['image-generation'],
    contextWindow: 0,
    costPerTokenInput: 0.03 / 1_000_000,
    costPerTokenOutput: 0.003 / 1_000_000,
    description: 'Popular text-to-image generation model with high quality outputs',
    bestFor: ['text-to-image', 'art generation', 'design'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'black-forest-labs/flux_1-kontext-dev',
    name: 'FLUX.1 Kontext Dev',
    publisher: 'Black Forest Labs',
    capabilities: ['image-generation'],
    contextWindow: 0,
    costPerTokenInput: 0.04 / 1_000_000,
    costPerTokenOutput: 0.004 / 1_000_000,
    description: 'Multimodal model for in-context image generation and editing',
    bestFor: ['image editing', 'context-aware generation', 'visual manipulation'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== 3D GENERATION MODELS ====================
  {
    id: 'microsoft/trellis',
    name: 'TRELLIS',
    publisher: 'Microsoft',
    capabilities: ['3d-generation', 'image-to-3d', 'text-to-3d'],
    contextWindow: 0,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0.005 / 1_000_000,
    description: '3D AI model that generates high-quality 3D assets from text or image inputs',
    bestFor: ['3D asset generation', 'game development', '3D modeling'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== VIDEO GENERATION MODELS ====================
  {
    id: 'nvidia/cosmos-transfer1-7b',
    name: 'Cosmos Transfer1 7B',
    publisher: 'NVIDIA',
    capabilities: ['video-generation', 'physical-ai'],
    contextWindow: 0,
    costPerTokenInput: 0.20 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Generates physics-aware video world states for physical AI development',
    bestFor: ['synthetic video generation', 'physical simulation', 'robotics training'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== AUTONOMOUS VEHICLE MODELS ====================
  {
    id: 'nvidia/streampetr',
    name: 'StreamPETR',
    publisher: 'NVIDIA',
    capabilities: ['autonomous-vehicles', '3d-detection', 'perception'],
    contextWindow: 0,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Efficient 3D object detection for autonomous driving with temporal propagation',
    bestFor: ['autonomous vehicles', '3D perception', 'temporal tracking'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/sparsedrive',
    name: 'SparseDrive',
    publisher: 'NVIDIA',
    capabilities: ['autonomous-vehicles', 'end-to-end', 'perception', 'planning'],
    contextWindow: 0,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'End-to-end autonomous driving stack with sparse scene representations',
    bestFor: ['autonomous driving', 'perception-prediction-planning', 'AV stack'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/bevformer',
    name: 'BEVFormer',
    publisher: 'NVIDIA',
    capabilities: ['autonomous-vehicles', 'perception', '3d-detection'],
    contextWindow: 0,
    costPerTokenInput: 0.12 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Advanced transformer for multi-frame bird\'s-eye-view 3D perception',
    bestFor: ['BEV perception', 'autonomous vehicles', 'multi-frame detection'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== LIFE SCIENCES MODELS ====================
  {
    id: 'nvidia/genmol-generate',
    name: 'GenMol',
    publisher: 'NVIDIA',
    capabilities: ['biology', 'chemistry', 'molecule-generation'],
    contextWindow: 512,
    costPerTokenInput: 0.20 / 1_000_000,
    costPerTokenOutput: 0.02 / 1_000_000,
    description: 'Fragment-based molecular generation by discrete diffusion',
    bestFor: ['drug discovery', 'molecule design', 'chemistry'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'nvidia/molmim-generate',
    name: 'MolMIM',
    publisher: 'NVIDIA',
    capabilities: ['biology', 'chemistry', 'molecule-generation'],
    contextWindow: 512,
    costPerTokenInput: 0.25 / 1_000_000,
    costPerTokenOutput: 0.025 / 1_000_000,
    description: 'Controlled generation of molecules with desired properties',
    bestFor: ['property optimization', 'drug discovery', 'targeted molecules'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'mit/diffdock',
    name: 'DiffDock',
    publisher: 'MIT',
    capabilities: ['biology', 'chemistry', 'docking'],
    contextWindow: 0,
    costPerTokenInput: 0.30 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Predicts 3D structure of molecular-protein interactions',
    bestFor: ['protein docking', 'drug-target interaction', 'structural biology'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'mit/boltz2',
    name: 'Boltz-2',
    publisher: 'MIT',
    capabilities: ['biology', 'protein-folding'],
    contextWindow: 0,
    costPerTokenInput: 0.25 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Predicts complex biomolecular structures',
    bestFor: ['protein folding', 'structure prediction', 'structural biology'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'openfold/openfold3',
    name: 'OpenFold3',
    publisher: 'OpenFold',
    capabilities: ['biology', 'protein-folding', 'drug-discovery'],
    contextWindow: 0,
    costPerTokenInput: 0.35 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Third-generation biomolecular foundation model for structure prediction',
    bestFor: ['protein complexes', 'drug discovery', 'biomolecular structures'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'ipd/proteinmpnn',
    name: 'ProteinMPNN',
    publisher: 'IPD',
    capabilities: ['biology', 'protein-generation'],
    contextWindow: 0,
    costPerTokenInput: 0.20 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Deep learning for predicting amino acid sequences for protein backbones',
    bestFor: ['protein design', 'sequence prediction', 'synthetic biology'],
    supportsTools: false,
    supportsStreaming: false,
  },
  {
    id: 'colabfold/msa-search',
    name: 'MSA Search',
    publisher: 'ColabFold',
    capabilities: ['biology', 'sequence-alignment'],
    contextWindow: 0,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Generates multiple sequence alignment from query sequence and database',
    bestFor: ['sequence alignment', 'evolutionary analysis', 'protein research'],
    supportsTools: false,
    supportsStreaming: false,
  },

  // ==================== AUDIO/VISUAL MODELS ====================
  {
    id: 'nvidia/audio2face-3d',
    name: 'Audio2Face 3D',
    publisher: 'NVIDIA',
    capabilities: ['speech-to-animation', 'digital-humans'],
    contextWindow: 0,
    costPerTokenInput: 0.10 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Converts streamed audio to facial blendshapes for realtime lipsyncing',
    bestFor: ['digital humans', 'lipsync', 'facial animation', 'realtime performance'],
    supportsTools: false,
    supportsStreaming: true,
  },
  {
    id: 'nvidia/background-noise-removal',
    name: 'Background Noise Removal',
    publisher: 'NVIDIA',
    capabilities: ['speech-enhancement', 'audio-processing'],
    contextWindow: 0,
    costPerTokenInput: 0.05 / 1_000_000,
    costPerTokenOutput: 0,
    description: 'Removes unwanted noises from audio improving speech intelligibility',
    bestFor: ['speech enhancement', 'noise reduction', 'call centers', 'meetings'],
    supportsTools: false,
    supportsStreaming: true,
  },

  // ==================== OPENUSD/DIGITAL TWINS MODELS ====================
  {
    id: 'nvidia/usdcode',
    name: 'USD Code',
    publisher: 'NVIDIA',
    capabilities: ['chat', 'code', 'OpenUSD'],
    contextWindow: 32_000,
    costPerTokenInput: 0.15 / 1_000_000,
    costPerTokenOutput: 0.15 / 1_000_000,
    description: 'State-of-the-art LLM for OpenUSD knowledge and USD-Python code generation',
    bestFor: ['OpenUSD development', 'digital twins', 'synthetic data generation'],
    supportsTools: true,
    supportsStreaming: true,
  },
];

// Helper functions
export function getNvidiaModelById(id: string): NvidiaModelConfig | undefined {
  return NVIDIA_MODEL_REGISTRY.find((m) => m.id === id);
}

export function getNvidiaModelsByCapability(cap: NvidiaModelCapability): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes(cap));
}

export function getNvidiaModelsByPublisher(publisher: string): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.publisher.toLowerCase() === publisher.toLowerCase());
}

export function getNvidiaChatModels(): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes('chat') && m.supportsTools);
}

export function getNvidiaCodeModels(): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes('code'));
}

export function getNvidiaVisionModels(): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes('vision') || m.multimodal);
}

export function getNvidiaEmbeddingModels(): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes('embedding'));
}

export function getNvidiaSafetyModels(): NvidiaModelConfig[] {
  return NVIDIA_MODEL_REGISTRY.filter((m) => m.capabilities.includes('safety'));
}

export function getCheapestNvidiaModel(): NvidiaModelConfig {
  return NVIDIA_MODEL_REGISTRY.reduce((cheapest, model) => {
    const modelCost = model.costPerTokenInput + model.costPerTokenOutput;
    const cheapestCost = cheapest.costPerTokenInput + cheapest.costPerTokenOutput;
    return modelCost < cheapestCost ? model : cheapest;
  });
}

export function getLargestContextNvidiaModel(): NvidiaModelConfig {
  return NVIDIA_MODEL_REGISTRY.reduce((largest, model) => 
    model.contextWindow > largest.contextWindow ? model : largest
  );
}

export function estimateNvidiaCost(modelId: string, inputTokens: number, outputTokens: number): { usd: number; breakdown: string } {
  const model = getNvidiaModelById(modelId);
  if (!model) {
    throw new Error(`Model ${modelId} not found in NVIDIA registry`);
  }
  
  const inputCost = inputTokens * model.costPerTokenInput;
  const outputCost = outputTokens * model.costPerTokenOutput;
  const total = inputCost + outputCost;
  
  return {
    usd: total,
    breakdown: `${model.name}: ${inputTokens} input + ${outputTokens} output tokens`,
  };
}

// Model capability tags for UI
export const MODEL_CAPABILITY_LABELS: Record<NvidiaModelCapability, { label: string; color: string; icon: string }> = {
  'chat': { label: 'Chat', color: 'blue', icon: '' },
  'code': { label: 'Code', color: 'green', icon: '' },
  'vision': { label: 'Vision', color: 'purple', icon: '' },
  'multimodal': { label: 'Multimodal', color: 'orange', icon: '' },
  'reasoning': { label: 'Reasoning', color: 'red', icon: '' },
  'embedding': { label: 'Embedding', color: 'gray', icon: '' },
  'image-generation': { label: 'Image Gen', color: 'pink', icon: '' },
  'speech-to-text': { label: 'Speech-to-Text', color: 'cyan', icon: '' },
  'text-to-speech': { label: 'Text-to-Speech', color: 'teal', icon: '' },
  'translation': { label: 'Translation', color: 'indigo', icon: '' },
  'reranking': { label: 'Reranking', color: 'yellow', icon: '' },
  'safety': { label: 'Safety', color: 'red', icon: '' },
  'agentic': { label: 'Agentic', color: 'violet', icon: '' },
  'long-context': { label: 'Long Context', color: 'amber', icon: '' },
  'function-calling': { label: 'Function Calling', color: 'emerald', icon: '' },
  'tool-calling': { label: 'Tool Calling', color: 'lime', icon: '' },
  'multilingual': { label: 'Multilingual', color: 'sky', icon: '' },
  'OCR': { label: 'OCR', color: 'slate', icon: '' },
  'text-generation': { label: 'Text Gen', color: 'blue', icon: '' },
  'math': { label: 'Math', color: 'orange', icon: '' },
  'speech-recognition': { label: 'Speech Recognition', color: 'cyan', icon: '' },
  'instruction-following': { label: 'Instruction Following', color: 'green', icon: '' },
  'summarization': { label: 'Summarization', color: 'teal', icon: '' },
  'security': { label: 'Security', color: 'red', icon: '' },
  '3d-generation': { label: '3D Generation', color: 'purple', icon: '' },
  'image-to-3d': { label: 'Image to 3D', color: 'purple', icon: '' },
  'text-to-3d': { label: 'Text to 3D', color: 'purple', icon: '' },
  'video-generation': { label: 'Video Gen', color: 'pink', icon: '' },
  'physical-ai': { label: 'Physical AI', color: 'amber', icon: '' },
  'autonomous-vehicles': { label: 'Autonomous Vehicles', color: 'blue', icon: '' },
  'end-to-end': { label: 'End-to-End', color: 'green', icon: '' },
  'perception': { label: 'Perception', color: 'violet', icon: '' },
  '3d-detection': { label: '3D Detection', color: 'indigo', icon: '' },
  'planning': { label: 'Planning', color: 'emerald', icon: '' },
  'biology': { label: 'Biology', color: 'green', icon: '' },
  'chemistry': { label: 'Chemistry', color: 'yellow', icon: '' },
  'molecule-generation': { label: 'Molecule Gen', color: 'lime', icon: '' },
  'docking': { label: 'Docking', color: 'teal', icon: '' },
  'protein-folding': { label: 'Protein Folding', color: 'green', icon: '' },
  'drug-discovery': { label: 'Drug Discovery', color: 'red', icon: '' },
  'protein-generation': { label: 'Protein Gen', color: 'emerald', icon: '' },
  'sequence-alignment': { label: 'Sequence Alignment', color: 'blue', icon: '' },
  'speech-to-animation': { label: 'Speech to Animation', color: 'pink', icon: '' },
  'digital-humans': { label: 'Digital Humans', color: 'violet', icon: '' },
  'speech-enhancement': { label: 'Speech Enhancement', color: 'cyan', icon: '' },
  'audio-processing': { label: 'Audio Processing', color: 'teal', icon: '' },
  'OpenUSD': { label: 'OpenUSD', color: 'orange', icon: '' },
};

// Model categories for grouping in UI
export const MODEL_CATEGORIES = {
  'Llama Family': ['Meta'],
  'Mistral Family': ['Mistral AI'],
  'DeepSeek': ['DeepSeek AI'],
  'Nemotron': ['NVIDIA'],
  'Kimi': ['Moonshot AI'],
  'Qwen': ['Qwen'],
  'Specialized': ['IBM', 'Google', 'OpenAI', 'ByteDance', 'MiniMax', 'Z-AI', 'THUDM', 'Sarvam AI', 'Microsoft'],
  'Regional AI': ['Stockmark', 'SpeakLeash', 'OpenGPT-X', 'Utter Project', 'Goto Company'],
  'Embeddings': ['NVIDIA'], // Will be filtered by capability
  'Safety': ['NVIDIA', 'Meta'], // Will be filtered by capability
  'Document AI': ['NVIDIA', 'Baidu'],
  'Speech': ['NVIDIA'],
  'Vision & Physical AI': ['NVIDIA'],
  'Creative': ['Stability AI', 'Black Forest Labs', 'Microsoft'],
  'Autonomous Vehicles': ['NVIDIA'],
  'Life Sciences': ['NVIDIA', 'MIT', 'OpenFold', 'IPD', 'ColabFold'],
  'Audio/Visual': ['NVIDIA'],
  'Digital Twins': ['NVIDIA'],
} as const;
