import { env } from '../config/env.js';
import logger from '../middleware/logger.js';

export interface MockAIResponse {
  content: string;
  model: string;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

/** Options for mock AI response generation */
export interface MockAIOptions {
  model?: string;
  systemPrompt?: string;
  maxTokens?: number;
  temperature?: number;
}

/**
 * Mock AI Service - Returns realistic placeholder responses
 * Perfect for zero-config exploration and UI testing
 */
export class MockAIService {
  private static instance: MockAIService;
  private responseDelay = 500; // ms to simulate network latency

  private constructor() {}

  static getInstance(): MockAIService {
    if (!MockAIService.instance) {
      MockAIService.instance = new MockAIService();
    }
    return MockAIService.instance;
  }

  /**
   * Check if mock mode is enabled
   */
  isEnabled(): boolean {
    return env.MOCK_AI_MODE;
  }

  /**
   * Generate a mock response based on the prompt
   */
  async generateResponse(prompt: string, options: MockAIOptions = {}): Promise<MockAIResponse> {
    const startTime = Date.now();

    // Simulate network delay
    await this.delay(this.responseDelay + Math.random() * 1000);

    // Analyze prompt to determine response type
    const response = this.craftResponse(prompt, options);

    logger.info(
      {
        mock: true,
        duration: Date.now() - startTime,
        promptLength: prompt.length,
        responseLength: response.content.length,
      },
      'Mock AI generated response'
    );

    return response;
  }

  /**
   * Stream a mock response (for SSE compatibility)
   */
  async *streamResponse(prompt: string, options: MockAIOptions = {}): AsyncGenerator<string> {
    const response = await this.generateResponse(prompt, options);
    const words = response.content.split(' ');

    // Simulate streaming by yielding words gradually
    for (let i = 0; i < words.length; i++) {
      await this.delay(50 + Math.random() * 100);
      yield words[i] + (i < words.length - 1 ? ' ' : '');
    }
  }

  /**
   * Craft an appropriate response based on prompt content
   */
  private craftResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const lowerPrompt = prompt.toLowerCase();

    // Architecture mode detection
    if (lowerPrompt.includes('architecture') || lowerPrompt.includes('design')) {
      return this.getArchitectureResponse(prompt, options);
    }

    // Code generation detection
    if (
      lowerPrompt.includes('code') ||
      lowerPrompt.includes('function') ||
      lowerPrompt.includes('implement')
    ) {
      return this.getCodeResponse(prompt, options);
    }

    // Diagram generation
    if (
      lowerPrompt.includes('diagram') ||
      lowerPrompt.includes('flowchart') ||
      lowerPrompt.includes('mermaid')
    ) {
      return this.getDiagramResponse(prompt, options);
    }

    // PRD/spec generation
    if (
      lowerPrompt.includes('prd') ||
      lowerPrompt.includes('spec') ||
      lowerPrompt.includes('requirements')
    ) {
      return this.getPRDResponse(prompt, options);
    }

    // Ship/deployment
    if (
      lowerPrompt.includes('ship') ||
      lowerPrompt.includes('deploy') ||
      lowerPrompt.includes('docker')
    ) {
      return this.getShipResponse(prompt, options);
    }

    // Default chat response
    return this.getChatResponse(prompt, options);
  }

  private getArchitectureResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const content = `# Architecture Design: Sample System

## Overview
This is a mock response demonstrating G-Rump's architecture mode. In real mode, this would be a complete system design based on your requirements.

## Components
- **Frontend**: Svelte 5 + Vite application
- **Backend**: Express API server with multi-agent orchestration
- **Database**: SQLite (dev) / PostgreSQL (prod)
- **AI Layer**: Multi-provider model router

## Architecture Diagram
\`\`\`mermaid
graph TB
    User[User] -->|HTTP| Frontend
    Frontend -->|API| Backend
    Backend -->|Query| Database
    Backend -->|Generate| AI[AI Providers]
    AI -->|Response| Backend
\`\`\`

## Data Flow
1. User submits natural language request
2. Intent compiler parses requirements
3. Multi-agent system processes request
4. Response streamed back to user

---
*This is a mock response. Add a real AI provider (e.g., NVIDIA_NIM_API_KEY) for actual architecture generation.*`;

    return this.createResponse(content, options.model || 'mock-architecture');
  }

  private getCodeResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const functionName = this.extractFunctionName(prompt) || 'exampleFunction';

    const content = `// Mock Code Response
// In real mode, this would be production-ready code based on your prompt

/**
 * Example function generated by G-Rump
 * Prompt: "${prompt.substring(0, 50)}..."
 */
export function ${functionName}(data: any): Promise<any> {
  console.log('Processing request for ${functionName} with data:', data);
  
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve({ 
        success: true, 
        message: 'Mock response - add API key for real code generation' 
      });
    }, 100);
  });
}

// Usage example:
// const result = await ${functionName}({ test: true });

/*
NOTES:
- This is a placeholder response in MOCK_AI_MODE
- To get real code generation, add an AI provider API key
- Supported providers: NVIDIA NIM, OpenRouter, Groq, Together AI
*/`;

    return this.createResponse(content, options.model || 'mock-code');
  }

  private extractFunctionName(prompt: string): string | null {
    const lowerPrompt = prompt.toLowerCase();
    const match = lowerPrompt.match(/(?:function|named|called)\s+`?(\w+)/);
    if (match && match[1]) {
      return match[1];
    }
    return null;
  }

  private getDiagramResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const content = `\`\`\`mermaid
flowchart TD
    Start([Start]) --> Input[User Input]
    Input --> Parse{Parse Intent}
    Parse -->|Success| Route[Route to Agent]
    Parse -->|Fail| Error[Show Error]
    Route --> Process[Process Request]
    Process --> Stream[Stream Response]
    Stream --> End([End])
    Error --> End
    
    style Start fill:#90EE90
    style End fill:#FFB6C1
\`\`\`

**Note**: This is a mock diagram. With a real AI provider, this would be a custom diagram based on your specific requirements.`;

    return this.createResponse(content, options.model || 'mock-diagram');
  }

  private getPRDResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const content = `# Product Requirements Document (Mock)

## 1. Executive Summary
This is a sample PRD showing G-Rump's PRD generation capability. With a real AI provider, this would contain detailed requirements.

## 2. Objectives
- Demonstrate PRD generation feature
- Show document structure
- Provide example format

## 3. User Stories
- **As a** developer, **I want** to generate PRDs from prompts **so that** I can document requirements quickly
- **As a** user, **I want** mock mode **so that** I can test without API keys

## 4. Technical Requirements
- Support for multiple output formats
- Integration with version control
- Export to PDF/Markdown

## 5. Acceptance Criteria
- [ ] PRD generates in under 30 seconds
- [ ] Document is well-structured
- [ ] Technical details are accurate

---
âš ï¸ **Mock Mode**: Add NVIDIA_NIM_API_KEY or OPENROUTER_API_KEY for real PRD generation.`;

    return this.createResponse(content, options.model || 'mock-prd');
  }

  private getShipResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const content = `# Deployment Configuration (Mock)

## Docker Setup
\`\`\`dockerfile
# This is a placeholder Dockerfile
# Real deployment config would be generated with an AI provider

FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
EXPOSE 3000
CMD ["node", "dist/index.js"]
\`\`\`

## Docker Compose
\`\`\`yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      # Add your API keys here
\`\`\`

## Next Steps
1. Add your AI provider API key
2. Run: npm run build
3. Deploy with: docker-compose up -d

---
ðŸ’¡ **Tip**: In mock mode, deployment files are templates. Enable real AI for production-ready configs.`;

    return this.createResponse(content, options.model || 'mock-ship');
  }

  private getChatResponse(prompt: string, options: MockAIOptions): MockAIResponse {
    const responses = [
      `ðŸ‘‹ Welcome to G-Rump! I'm currently running in **mock mode**, which means I'm generating realistic placeholder responses instead of using real AI.

**What this means:**
- You can explore the UI and features immediately
- No API keys required
- Responses are templates showing what real output looks like

**To enable real AI:**
1. Get a free API key from https://build.nvidia.com/ (NVIDIA NIM)
2. Add it to \\\`backend/.env\\\`: \\\`NVIDIA_NIM_API_KEY=your_key_here\\\`
3. Set \\\`MOCK_AI_MODE=false\\\`
4. Restart the server

**Quick Commands:**
- \\\`npm run dev\\\` - Start everything
- \\\`npm run setup:interactive\\\` - Re-run this setup

How can I help you today?`,

      `I received your message: "${prompt.substring(0, 100)}${prompt.length > 100 ? '...' : ''}"

This is a **mock response** - the AI isn't actually processing your input right now.

**Why mock mode?**
âœ“ Test the UI without spending API credits  
âœ“ Develop offline  
âœ“ No configuration needed to start  

**Ready for real AI?**
Just add an API key from NVIDIA NIM, OpenRouter, or Groq to your \\\`backend/.env\\\` file and restart!`,

      `ðŸ¤– **G-Rump Mock Mode Active**

Your prompt has been received and would normally be processed by the multi-agent system with these steps:

1. **Intent Analysis** - Parse your natural language request
2. **Agent Selection** - Route to appropriate specialist (Architect, Frontend, Backend, etc.)
3. **Processing** - Generate code, architecture, or documentation
4. **Response Streaming** - Real-time results

**Current Status:** Mock responses enabled  
**Solution:** Add \\\`NVIDIA_NIM_API_KEY\\\` to \\\`backend/.env\\\` for real AI`,
    ];

    const randomResponse = responses[Math.floor(Math.random() * responses.length)];
    return this.createResponse(randomResponse, options.model || 'mock-chat');
  }

  private createResponse(content: string, model: string): MockAIResponse {
    const promptTokens = Math.floor(content.length / 4);
    const completionTokens = Math.floor(content.length / 4);

    return {
      content,
      model,
      usage: {
        prompt_tokens: promptTokens,
        completion_tokens: completionTokens,
        total_tokens: promptTokens + completionTokens,
      },
    };
  }

  private delay(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

// Export singleton instance
export const mockAI = MockAIService.getInstance();
