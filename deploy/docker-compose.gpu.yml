version: '3.8'

# GPU-enabled overlay for G-Rump
# 
# Usage: docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
# 
# Prerequisites:
# - NVIDIA GPU with CUDA support
# - nvidia-container-toolkit installed
# - Docker configured with nvidia runtime

services:
  backend:
    # Override backend with GPU support
    environment:
      # Enable GPU acceleration
      - NVIDIA_NIM_ENABLED=true
      - NVIDIA_NIM_URL=http://nim:8000
      - NIM_EMBED_BATCH_SIZE=256
      - NIM_EMBED_MAX_WAIT_MS=50
      - GPU_ENABLED=true
    depends_on:
      redis:
        condition: service_healthy
      nim:
        condition: service_healthy

  nim:
    # NVIDIA NIM for GPU-accelerated inference
    image: nvcr.io/nvidia/nim:latest
    container_name: grump-nim
    ports:
      - "8000:8000"
    environment:
      - NGC_API_KEY=${NGC_API_KEY:-}
      - NIM_MODEL_NAME=meta/llama-3.1-8b-instruct
      - NIM_MAX_BATCH_SIZE=256
      - NIM_MAX_CONCURRENT_REQUESTS=64
    volumes:
      - nim-cache:/opt/nim/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - grump-network
    restart: unless-stopped

  # GPU-accelerated embedding service (optional)
  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.2
    container_name: grump-embeddings
    ports:
      - "8001:80"
    environment:
      - MODEL_ID=BAAI/bge-small-en-v1.5
      - MAX_BATCH_SIZE=64
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - grump-network
    restart: unless-stopped

volumes:
  nim-cache:
    driver: local
